import{_ as e}from"./plugin-vue_export-helper-DlAUqK2U.js";import{c as t,o as a,a as o}from"./app-DQ7uMTFd.js";const i={},p=o('<h2 id="介绍" tabindex="-1"><a class="header-anchor" href="#介绍"><span>介绍</span></a></h2><p>2024 年 5 月 13 日, Openai 宣布推出 GPT-4o，这是他们的新旗舰模型，可以实时对音频、视觉和文本进行推理。</p><p>GPT-4o（“o”代表“o​​mni”）是迈向更自然的人机交互的一步——它接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在短至 232 毫秒的时间内响应音频输入，平均为 <code>320</code> 毫秒，与<a href="https://www.pnas.org/doi/10.1073/pnas.0903616106" target="_blank" rel="noopener noreferrer">人类的响应时间相似（在新窗口中打开）</a>在一次谈话中。它在英语文本和代码上的性能与 GPT-4 Turbo 的性能相匹配，在非英语文本上的性能显着提高，同时 API 的速度也更快，成本降低了 50%。与现有模型相比，GPT-4o 在<code>视觉和音频理解</code>方面尤其出色。</p><h2 id="模型能力" tabindex="-1"><a class="header-anchor" href="#模型能力"><span>模型能力</span></a></h2><ul><li>音频：在GPT-4o之前也可以使用语言模式，但延迟无法做到毫秒级</li></ul><p>语音模式：平均延迟为 2.8 秒 (GPT-3.5) 和 5.4 秒 (GPT-4)</p><blockquote><p>语音模式是由三个独立模型组成的管道：一个简单模型将音频转录为文本，GPT-3.5 或 GPT-4 接收文本并输出文本，第三个简单模型将该文本转换回音频。这个过程意味着主要智能来源GPT-4丢失了大量信息——它无法直接观察音调、多个说话者或背景噪音，也无法输出笑声、歌唱或表达情感。</p></blockquote><p>GPT-4o：平均延迟达到 320ms</p><blockquote><p>GPT-4o是通过跨文本、视觉和音频端到端地训练了一个新模型，这意味着所有输入和输出都由同一神经网络处理。由于 GPT-4o 是Openai第一个结合所有这些模式的模型，因此他们也仍然只是浅尝辄止地探索该模型的功能及其局限性。</p></blockquote><h3 id="探索能力" tabindex="-1"><a class="header-anchor" href="#探索能力"><span>探索能力</span></a></h3><ul><li><p>文本</p></li><li><p>视觉</p></li><li><p>音频</p></li><li><p>视频</p></li></ul><figure><img src="https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-33-56.496.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="模型可用性" tabindex="-1"><a class="header-anchor" href="#模型可用性"><span>模型可用性</span></a></h2><p>GPT-4o 的文本和图像功能今天开始在 ChatGPT 中推出。Openai正在免费套餐中提供 GPT-4o，并向 Plus 用户提供高达 5 倍的消息限制。将在未来几周内在 ChatGPT Plus 中推出新版本的语音模式 GPT-4o alpha。</p><p>开发人员现在还可以在 API 中访问 GPT-4o 作为文本和视觉模型。与 GPT-4 Turbo 相比，GPT-4o 速度提高 2 倍，价格降低一半，速率限制提高 5 倍。计划在未来几周内在 API 中向一小部分值得信赖的合作伙伴推出对 GPT-4o 新音频和视频功能的支持。</p><h2 id="体验" tabindex="-1"><a class="header-anchor" href="#体验"><span>体验</span></a></h2><p><a href="https://chatgpt.com/" target="_blank" rel="noopener noreferrer">地址</a> 特地去chatgpt官网看了下，结果发现还是没有GPT-4o这个模型可供选择。所以说明也不是所有人都可以免费使用，解释权在openai官方。看到网上那些夸张的标题我真的是吐了。</p><p>提示</p><figure><img src="https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-18.798.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>对话窗口</p><figure><img src="https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-34.582.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>发现只有3.5可用</p><figure><img src="https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-55.761.png" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure>',23),n=[p];function r(c,l){return a(),t("div",null,n)}const h=e(i,[["render",r],["__file","GPT_4o.html.vue"]]),u=JSON.parse('{"path":"/article/ai/GPT_4o.html","title":"你好 GPT-4o","lang":"zh-CN","frontmatter":{"title":"你好 GPT-4o","date":"2024-05-14T00:00:00.000Z","categories":["AI对话"],"tags":["GPT-4o"],"description":"介绍 2024 年 5 月 13 日, Openai 宣布推出 GPT-4o，这是他们的新旗舰模型，可以实时对音频、视觉和文本进行推理。 GPT-4o（“o”代表“o​​mni”）是迈向更自然的人机交互的一步——它接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在短至 232 毫秒的时间内响应音频输入，平均为 ...","head":[["meta",{"property":"og:url","content":"https://lianyp.fun/article/ai/GPT_4o.html"}],["meta",{"property":"og:site_name","content":"小道空间-Vuepress开源轻博客系统"}],["meta",{"property":"og:title","content":"你好 GPT-4o"}],["meta",{"property":"og:description","content":"介绍 2024 年 5 月 13 日, Openai 宣布推出 GPT-4o，这是他们的新旗舰模型，可以实时对音频、视觉和文本进行推理。 GPT-4o（“o”代表“o​​mni”）是迈向更自然的人机交互的一步——它接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在短至 232 毫秒的时间内响应音频输入，平均为 ..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-33-56.496.png"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2024-05-20T09:04:18.000Z"}],["meta",{"property":"article:author","content":"梁小道"}],["meta",{"property":"article:tag","content":"GPT-4o"}],["meta",{"property":"article:published_time","content":"2024-05-14T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2024-05-20T09:04:18.000Z"}],["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"你好 GPT-4o\\",\\"image\\":[\\"https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-33-56.496.png\\",\\"https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-18.798.png\\",\\"https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-34.582.png\\",\\"https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-55.761.png\\"],\\"datePublished\\":\\"2024-05-14T00:00:00.000Z\\",\\"dateModified\\":\\"2024-05-20T09:04:18.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"梁小道\\",\\"url\\":\\"https://lianyp.fun\\"}]}"]]},"headers":[{"level":2,"title":"介绍","slug":"介绍","link":"#介绍","children":[]},{"level":2,"title":"模型能力","slug":"模型能力","link":"#模型能力","children":[{"level":3,"title":"探索能力","slug":"探索能力","link":"#探索能力","children":[]}]},{"level":2,"title":"模型可用性","slug":"模型可用性","link":"#模型可用性","children":[]},{"level":2,"title":"体验","slug":"体验","link":"#体验","children":[]}],"git":{"createdTime":1716002958000,"updatedTime":1716195858000,"contributors":[{"name":"liangyp","email":"2267841523@qq.com","commits":2}]},"readingTime":{"minutes":2.71,"words":812},"filePathRelative":"article/ai/GPT_4o.md","localizedDate":"2024年5月14日","excerpt":"<h2>介绍</h2>\\n<p>2024 年 5 月 13 日, Openai 宣布推出 GPT-4o，这是他们的新旗舰模型，可以实时对音频、视觉和文本进行推理。</p>\\n<p>GPT-4o（“o”代表“o​​mni”）是迈向更自然的人机交互的一步——它接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在短至 232 毫秒的时间内响应音频输入，平均为 <code>320</code> 毫秒，与<a href=\\"https://www.pnas.org/doi/10.1073/pnas.0903616106\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">人类的响应时间相似（在新窗口中打开）</a>在一次谈话中。它在英语文本和代码上的性能与 GPT-4 Turbo 的性能相匹配，在非英语文本上的性能显着提高，同时 API 的速度也更快，成本降低了 50%。与现有模型相比，GPT-4o 在<code>视觉和音频理解</code>方面尤其出色。</p>","autoDesc":true}');export{h as comp,u as data};
