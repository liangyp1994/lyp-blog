---
title: 你好 GPT-4o
date: 2024/05/14
categories:
- AI
---

## 介绍

2024 年 5 月 13 日, Openai 宣布推出 GPT-4o，这是他们的新旗舰模型，可以实时对音频、视觉和文本进行推理。

GPT-4o（“o”代表“o​​mni”）是迈向更自然的人机交互的一步——它接受文本、音频、图像和视频的任意组合作为输入，并生成文本、音频和图像的任意组合输出。它可以在短至 232 毫秒的时间内响应音频输入，平均为 `320` 毫秒，与[人类的响应时间相似（在新窗口中打开）](https://www.pnas.org/doi/10.1073/pnas.0903616106)在一次谈话中。它在英语文本和代码上的性能与 GPT-4 Turbo 的性能相匹配，在非英语文本上的性能显着提高，同时 API 的速度也更快，成本降低了 50%。与现有模型相比，GPT-4o 在`视觉和音频理解`方面尤其出色。

## 模型能力

- 音频：在GPT-4o之前也可以使用语言模式，但延迟无法做到毫秒级

语音模式：平均延迟为 2.8 秒 (GPT-3.5) 和 5.4 秒 (GPT-4)

> 语音模式是由三个独立模型组成的管道：一个简单模型将音频转录为文本，GPT-3.5 或 GPT-4 接收文本并输出文本，第三个简单模型将该文本转换回音频。这个过程意味着主要智能来源GPT-4丢失了大量信息——它无法直接观察音调、多个说话者或背景噪音，也无法输出笑声、歌唱或表达情感。

GPT-4o：平均延迟达到 320ms

> GPT-4o是通过跨文本、视觉和音频端到端地训练了一个新模型，这意味着所有输入和输出都由同一神经网络处理。由于 GPT-4o 是Openai第一个结合所有这些模式的模型，因此他们也仍然只是浅尝辄止地探索该模型的功能及其局限性。

### 探索能力

- 文本

- 视觉

- 音频

- 视频

![](https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-33-56.496.png)

## 模型可用性

GPT-4o 的文本和图像功能今天开始在 ChatGPT 中推出。Openai正在免费套餐中提供 GPT-4o，并向 Plus 用户提供高达 5 倍的消息限制。将在未来几周内在 ChatGPT Plus 中推出新版本的语音模式 GPT-4o alpha。

开发人员现在还可以在 API 中访问 GPT-4o 作为文本和视觉模型。与 GPT-4 Turbo 相比，GPT-4o 速度提高 2 倍，价格降低一半，速率限制提高 5 倍。计划在未来几周内在 API 中向一小部分值得信赖的合作伙伴推出对 GPT-4o 新音频和视频功能的支持。

## 体验

[地址](https://chatgpt.com/) 特地去chatgpt官网看了下，结果发现还是没有GPT-4o这个模型可供选择。所以说明也不是所有人都可以免费使用，解释权在openai官方。看到网上那些夸张的标题我真的是吐了。

提示

![](https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-18.798.png)

对话窗口

![](https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-34.582.png)

发现只有3.5可用

![](https://lianyp.fun/picture/mark-text-doc/picture/2024/05/16/screen-2024-05-16_09-40-55.761.png)
